{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Untitled6.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 라이브러리 및 데이터 불러오기\n",
    "# 필요한 라이브러리를 불러온다.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "import os\n",
    "import imageio\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "### GAN의 생성자(Generator)\n",
    "# 생성자는 랜덤 벡터 z를 입력으로 받아 가짜 이미지를 출력한다.\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    # 네트워크 구조\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(in_features=100, out_features=256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(in_features=512, out_features=1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(in_features=1024, out_features=28*28),\n",
    "            nn.Tanh())\n",
    "    \n",
    "    # (batch_size x 100) 크기의 랜덤 벡터를 받아 \n",
    "    # 이미지를 (batch_size x 1 x 28 x 28) 크기로 출력한다.\n",
    "    def forward(self, inputs):\n",
    "        return self.main(inputs).view(-1, 1, 28, 28)\n",
    "\n",
    "### GAN의 구분자(Discriminator)\n",
    "# 구분자는 이미지를 입력으로 받아 이미지가 진짜인지 가짜인지 출력한다.\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    # 네트워크 구조\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(in_features=28*28, out_features=1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(inplace=True),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(inplace=True),\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=1),\n",
    "            nn.Sigmoid())\n",
    "    \n",
    "    # (batch_size x 1 x 28 x 28) 크기의 이미지를 받아\n",
    "    # 이미지가 진짜일 확률을 0~1 사이로 출력한다.\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.view(-1, 28*28)\n",
    "        return self.main(inputs)\n",
    "\n",
    "####################\n",
    "# 시각화 함수\n",
    "def show_generated_data(real_data, fake_data)\n",
    "  plt.figure(figsize=(15,5))\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.axis(\"off\")\n",
    "  plt.title(\"Real Images\")\n",
    "  plt.imshow(np.transpose(vutils.make_grid(real_data[:64], padding=5, normalize=True).cpu(), (1,2,0)))\n",
    "\n",
    "  #Plot fake image\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.axis(\"off\")\n",
    "  plt.title(\"Fake Images\")\n",
    "######################\n",
    "\n",
    "# 학습 결과 시각화하기\n",
    "\n",
    "\n",
    "def square_plot(data, path):\n",
    "    \"\"\"Take an array of shape (n, height, width) or (n, height, width , 3)\n",
    "       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\"\"\"\n",
    "\n",
    "    if type(data) == list:\n",
    "\t    data = np.concatenate(data)\n",
    "    # normalize data for display\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "    # force the number of filters to be square\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "\n",
    "    padding = (((0, n ** 2 - data.shape[0]) ,\n",
    "                (0, 1), (0, 1))  # add some space between filters\n",
    "               + ((0, 0),) * (data.ndim - 3))  # don't pad the last dimension (if there is one)\n",
    "    data = np.pad(data , padding, mode='constant' , constant_values=1)  # pad with ones (white)\n",
    "\n",
    "    # tilethe filters into an image\n",
    "    data = data.reshape((n , n) + data.shape[1:]).transpose((0 , 2 , 1 , 3) + tuple(range(4 , data.ndim + 1)))\n",
    "\n",
    "    data = data.reshape((n * data.shape[1] , n * data.shape[3]) + data.shape[4:])\n",
    "\n",
    "    plt.imsave(path, data, cmap='gray')\n",
    "\n",
    "# 데이터 전처리 방식을 지정한다.\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # 데이터를 PyTorch의 Tensor 형식으로 바꾼다.\n",
    "        transforms.Normalize(mean=(0.5,), std=(0.5,)) # 픽셀값 0 ~ 1 -> -1 ~ 1\n",
    "])\n",
    "\n",
    "# MNIST 데이터셋을 불러온다. 지정한 폴더에 없을 경우 자동으로 다운로드한다.\n",
    "mnist = datasets.MNIST(root='/content/gdrive/My Drive/AI/data', download=True, transform=transform)\n",
    "\n",
    "# 데이터를 한번에 batch_size만큼만 가져오는 dataloader를 만든다.\n",
    "dataloader = DataLoader(mnist, batch_size=60, shuffle=True)\n",
    "\n",
    "###############\n",
    "use_gpu = False\n",
    "if torch.cuda.is_available():\n",
    "    use_gpu = True\n",
    "leave_log = True\n",
    "if leave_log:\n",
    "    result_dir = '/content/gdrive/My Drive/AI/GAN_generated_images'\n",
    "    if not os.path.isdir(result_dir):\n",
    "        os.mkdir(result_dir)\n",
    "\n",
    "### 생성자와 구분자 객체 만들기\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "\n",
    "if use_gpu:\n",
    "    G.cuda()\n",
    "    D.cuda()\n",
    "\n",
    "### 손실 함수와 최적화 기법 지정하기\n",
    "# Binary Cross Entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 생성자의 매개 변수를 최적화하는 Adam optimizer\n",
    "G_optimizer = Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# 구분자의 매개 변수를 최적화하는 Adam optimizer\n",
    "D_optimizer = Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "if leave_log:\n",
    "    train_hist = {}\n",
    "    train_hist['D_losses'] = []\n",
    "    train_hist['G_losses'] = []\n",
    "    generated_images = []\n",
    "    \n",
    "#################\n",
    "\n",
    "z_fixed = Variable(torch.randn(5 * 5, 100), volatile=True)\n",
    "if use_gpu:\n",
    "    z_fixed = z_fixed.cuda()\n",
    "\n",
    "\n",
    "\n",
    "################################3\n",
    "\n",
    "### 모델 학습을 위한 반복문\n",
    "# 데이터셋을 100번 돌며 학습한다.\n",
    "for epoch in range(100):\n",
    "    \n",
    "    if leave_log:\n",
    "        D_losses = []\n",
    "        G_losses = []\n",
    "    \n",
    "    # 한번에 batch_size만큼 데이터를 가져온다.\n",
    "    for real_data, _ in dataloader:\n",
    "        batch_size = real_data.size(0)\n",
    "        \n",
    "        # 데이터를 pytorch의 변수로 변환한다.\n",
    "        real_data = Variable(real_data)\n",
    "\n",
    "        ### 구분자 학습시키기\n",
    "\n",
    "        # 이미지가 진짜일 때 정답 값은 1이고 가짜일 때는 0이다.\n",
    "        # 정답지에 해당하는 변수를 만든다.\n",
    "        target_real = Variable(torch.ones(batch_size, 1))\n",
    "        target_fake = Variable(torch.zeros(batch_size, 1))\n",
    "         \n",
    "        if use_gpu:\n",
    "            real_data, target_real, target_fake = real_data.cuda(), target_real.cuda(), target_fake.cuda()\n",
    "            \n",
    "        # 진짜 이미지를 구분자에 넣는다.\n",
    "        D_result_from_real = D(real_data)\n",
    "        # 구분자의 출력값이 정답지인 1에서 멀수록 loss가 높아진다.\n",
    "        D_loss_real = criterion(D_result_from_real, target_real)\n",
    "\n",
    "        # 생성자에 입력으로 줄 랜덤 벡터 z를 만든다.\n",
    "        z = Variable(torch.randn((batch_size, 100)))\n",
    "        \n",
    "        if use_gpu:\n",
    "            z = z.cuda()\n",
    "            \n",
    "        # 생성자로 가짜 이미지를 생성한다.\n",
    "        fake_data = G(z)\n",
    "        \n",
    "        # 생성자가 만든 가짜 이미지를 구분자에 넣는다.\n",
    "        D_result_from_fake = D(fake_data)\n",
    "        # 구분자의 출력값이 정답지인 0에서 멀수록 loss가 높아진다.\n",
    "        D_loss_fake = criterion(D_result_from_fake, target_fake)\n",
    "        \n",
    "        # 구분자의 loss는 두 문제에서 계산된 loss의 합이다.\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "        \n",
    "        # 구분자의 매개 변수의 미분값을 0으로 초기화한다.\n",
    "        D.zero_grad()\n",
    "        # 역전파를 통해 매개 변수의 loss에 대한 미분값을 계산한다.\n",
    "        D_loss.backward()\n",
    "        # 최적화 기법을 이용해 구분자의 매개 변수를 업데이트한다.\n",
    "        D_optimizer.step()\n",
    "        \n",
    "        if leave_log:\n",
    "            D_losses.append(D_loss.data[0])\n",
    "\n",
    "        # train generator G\n",
    "\n",
    "        ### 생성자 학습시키기\n",
    "        \n",
    "        # 생성자에 입력으로 줄 랜덤 벡터 z를 만든다.\n",
    "        z = Variable(torch.randn((batch_size, 100)))\n",
    "        \n",
    "        if use_gpu:\n",
    "            z = z.cuda()\n",
    "        \n",
    "        # 생성자로 가짜 이미지를 생성한다.\n",
    "        fake_data = G(z)\n",
    "        # 생성자가 만든 가짜 이미지를 구분자에 넣는다.\n",
    "        D_result_from_fake = D(fake_data)\n",
    "        # 생성자의 입장에서 구분자의 출력값이 1에서 멀수록 loss가 높아진다.\n",
    "        G_loss = criterion(D_result_from_fake, target_real)\n",
    "        \n",
    "        # 생성자의 매개 변수의 미분값을 0으로 초기화한다.\n",
    "        G.zero_grad()\n",
    "        # 역전파를 통해 매개 변수의 loss에 대한 미분값을 계산한다.\n",
    "        G_loss.backward()\n",
    "        # 최적화 기법을 이용해 생성자의 매개 변수를 업데이트한다.\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        if leave_log:\n",
    "            G_losses.append(G_loss.data[0])\n",
    "    if leave_log:\n",
    "        true_positive_rate = (D_result_from_real > 0.5).float().mean().data[0]\n",
    "        true_negative_rate = (D_result_from_fake < 0.5).float().mean().data[0]\n",
    "        base_message = (\"Epoch: {epoch:<3d} D Loss: {d_loss:<8.6} G Loss: {g_loss:<8.6} \"\n",
    "                        \"True Positive Rate: {tpr:<5.1%} True Negative Rate: {tnr:<5.1%}\"\n",
    "                       )\n",
    "        message = base_message.format(\n",
    "                    epoch=epoch,\n",
    "                    d_loss=sum(D_losses)/len(D_losses),\n",
    "                    g_loss=sum(G_losses)/len(G_losses),\n",
    "                    tpr=true_positive_rate,\n",
    "                    tnr=true_negative_rate\n",
    "        )\n",
    "        print(message)\n",
    "    \n",
    "    if leave_log:\n",
    "        fake_data_fixed = G(z_fixed)\n",
    "        image_path = result_dir + '/epoch{}.png'.format(epoch)\n",
    "        square_plot(fake_data_fixed.view(25, 28, 28).cpu().data.numpy(), path=image_path)\n",
    "        generated_images.append(image_path)\n",
    "    \n",
    "    if leave_log:\n",
    "        train_hist['D_losses'].append(torch.mean(torch.FloatTensor(D_losses)))\n",
    "        train_hist['G_losses'].append(torch.mean(torch.FloatTensor(G_losses)))\n",
    "\n",
    "torch.save(G.state_dict(), \"/content/gdrive/My Drive/AI/gan_generator.pkl\")\n",
    "torch.save(D.state_dict(), \"/content/gdrive/My Drive/AI/gan_discriminator.pkl\")\n",
    "with open('/content/gdrive/My Drive/AI/gan_train_history.pkl', 'wb') as f:\n",
    "    pickle.dump(train_hist, f)\n",
    "\n",
    "generated_image_array = [imageio.imread(generated_image) for generated_image in generated_images]\n",
    "imageio.mimsave(result_dir + '/GAN_generation.gif', generated_image_array, fps=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}