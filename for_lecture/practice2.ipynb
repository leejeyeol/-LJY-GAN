{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **2. Numpy를 이용하여 Backpropagation 구현하기**\n",
    "목표 : numpy를 이용하여 xor 함수를 근사하는 multilayer perceptron을 backpropagation으로 학습해봅시다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# epoch은 모든 데이터를 전부 사용하여 한번 학습하는 것을 말합니다. epoch을 20000으로 두면 학습을 20000만번 진행합니다.\n",
    "epochs = 20000\n",
    "\n",
    "# 입력, 은닉, 출력 레이어의 노드 수입니다.\n",
    "inputSize, inputLayerSize, outputLayerSize = 2, 3, 1\n",
    "\n",
    "# weight 갱신 때 사용할 학습률입니다.\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 인공적으로 입력과 출력 만들기 X : 입력 t : 정답(label)\n",
    "# xor 데이터입니다.\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "t = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "\n",
    "# 활성화 함수로 사용할 sigmoid와 그 미분을 함수로 선언.\n",
    "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(x): return x * (1 - x)\n",
    "\n",
    "\n",
    "# weight matrix를 생성합니다. 수동으로 설정할 필요 없이 uniform 분포에서 샘플링한 랜덤값으로 초기화됩니다.\n",
    "W_layer_1 = np.random.uniform(size=(inputSize, inputLayerSize))\n",
    "W_layer_2 = np.random.uniform(size=(inputLayerSize, outputLayerSize))\n",
    "'''\n",
    "랜덤으로 만들기 때문에 값은 매번 다르지만, 다음과 같은 형태로 나옵니다.\n",
    "W_layer_1\n",
    "array([[0.2157437 , 0.88022068, 0.14593536],\n",
    "       [0.04643253, 0.21375877, 0.94012982]])\n",
    "W_layer_2\n",
    "array([[0.41842079],\n",
    "       [0.40828852],\n",
    "       [0.09722518]])\n",
    "'''\n",
    "Error_list = []\n",
    "for i in range(epochs):\n",
    "    # 순전파\n",
    "    layer_1_output = sigmoid(np.dot(X, W_layer_1))  # layer 1\n",
    "    layer_2_output = sigmoid(np.dot(layer_1_output, W_layer_2))  # layer 2\n",
    "\n",
    "    # loss function 계산. loss function으로 mean squared error를 사용하였습니다.\n",
    "    E = 1 / 2 * np.square(t - layer_2_output).sum()\n",
    "    Error_list.append(E)\n",
    "    \n",
    "    # 역전파 시작\n",
    "    layer_2_W_grad = (layer_2_output-t) * sigmoid_derivative(layer_2_output)\n",
    "    # delta layer 2(output layer) : partial E/h * partiral h/f (partial f/x 는 weight 갱신할 때 곱함)\n",
    "\n",
    "    layer_1_W_grad = layer_2_W_grad.dot(W_layer_2.T) * sigmoid_derivative(layer_1_output)\n",
    "    # delta layer 1 : partial E/(layer1의)h * partial h/f (partial f/x 는 weight 갱신할 때 곱함)\n",
    "\n",
    "    # weight 갱신. 갱신된 W = 기존 W - 출발노드의 output * 도착 노드의 delta\n",
    "    # 각각 partial f/x에 해당하는 해당 레이어로의 입력데이터 행렬이 행렬곱됩니다.\n",
    "    W_layer_2 -= learning_rate * np.dot(layer_1_output.T, layer_2_W_grad)\n",
    "    W_layer_1 -= learning_rate * np.dot(X.T, layer_1_W_grad)\n",
    "\n",
    "\n",
    "# 학습종료 후 결과물 확인 (optional)\n",
    "print(\"Input is\")\n",
    "print(X)\n",
    "print(\"expected output is\")\n",
    "print(t)\n",
    "print(\"actual output is \")\n",
    "print(layer_2_output)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(Error_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# 활성화 함수로 사용할 계단 함수 입니다.\n",
    "def step(x): return np.array(x>0, dtype=np.int)\n",
    "\n",
    "# 인공적으로 입력과 출력 만들기 X : 입력 Y : 정답(label)\n",
    "# and 데이터입니다.\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "# 2번실습에서 학습한 weight들을 넣어보겠습니다.\n",
    "W_layer_1 = np.array([[ 5.43873189, -3.11375556 , 5.29522101],\n",
    " [-3.07384198 , 5.53159189 , 5.30037098]])\n",
    "W_layer_2 = np.array([[-8.35124937],\n",
    " [-8.33291174],\n",
    " [11.69026761]])\n",
    "\n",
    "\n",
    "# 순전파\n",
    "layer_1_output = step(np.dot(X, W_layer_1)) # layer 1\n",
    "layer_2_output = step(np.dot(layer_1_output, W_layer_2)) # layer 2\n",
    "   \n",
    "# 학습종료 후 결과물 확인 (optional)\n",
    "print(\"Input is\")\n",
    "print(X)\n",
    "print(\"expected output is\")\n",
    "print(Y)\n",
    "print(\"actual output is \")\n",
    "print(layer_2_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}