{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# **12. CNN autoencoder + weight 저장, 불러오기, finetuing**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive/')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "#os.mkdir(\"/content/gdrive/My Drive/AI\") #폴더를 만드는 코드이니 한번만 실행하세요. 구글드라이브에서 직접 폴더 만들어도 됩니다.\n",
    "with open('/content/gdrive/My Drive/AI/hello.txt', 'w') as f:\n",
    "  f.write('Hello Google Drive colab !') # 테스트용 텍스트파일 생성\n",
    "!cat /content/gdrive/My\\ Drive/AI/hello.txt #텍스트 파일 내용 출력하기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MNIST_CNN_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return z\n",
    "\n",
    "\n",
    "class MNIST_CNN_Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x_ = self.decoder(z)\n",
    "        return x_\n",
    "\n",
    "\n",
    "class MNIST_FCN(nn.Module):\n",
    "    def __init__(self, class_num):\n",
    "        super().__init__()\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(32, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, self.class_num),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32)\n",
    "        y = self.fc_net(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    # 단일 라벨 텐서를 원핫 벡터로 바꿔줍니다.\n",
    "    y = torch.eye(num_classes)\n",
    "    one_hot = y[labels]\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def softmax_to_one_hot(tensor):\n",
    "    # softmax 결과를 가장 높은 값이 1이 되도록 하여 원핫 벡터로 바꿔줍니다. acuuracy 구할 때 씁니다.\n",
    "    max_idx = torch.argmax(tensor, 1, keepdim=True)\n",
    "    if tensor.is_cuda:\n",
    "        one_hot = torch.zeros(tensor.shape).cuda()\n",
    "    else:\n",
    "        one_hot = torch.zeros(tensor.shape)\n",
    "    one_hot.scatter_(1, max_idx, 1)\n",
    "    return one_hot\n",
    "\n",
    "def weight_init(m):\n",
    "    # Conv layer와 batchnorm layer를 위한 가중치 초기화를 추가함.\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# load the dataset\n",
    "dataset = datasets.MNIST('../data', train=True,\n",
    "                         download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "        , transforms.Normalize((0.5,), (0.5,))\n",
    "    ]))\n",
    "num_train = len(dataset)\n",
    "valid_size = 500\n",
    "\n",
    "indices = list(range(num_train))\n",
    "split = num_train - valid_size\n",
    "np.random.shuffle(indices)\n",
    "train_idx, valid_idx = indices[:split], indices[split:]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                           batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                           batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor()\n",
    "                       , transforms.Normalize((0.5,), (0.5,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "encoder = MNIST_CNN_Encoder().cuda()\n",
    "encoder.apply(weight_init)\n",
    "\n",
    "decoder = MNIST_CNN_Decoder().cuda()\n",
    "decoder.apply(weight_init)\n",
    "\n",
    "net_params = list(encoder.parameters())+list(decoder.parameters())\n",
    "optimizer = optim.Adam(net_params, betas=(0.5, 0.999),lr=learning_rate)\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "for epoch in range(epochs):\n",
    "    for i, (X, _) in enumerate(train_loader):\n",
    "        X = X.cuda()\n",
    "        z = encoder(X)\n",
    "        recon_X = decoder(z)\n",
    "\n",
    "        loss = loss_function(recon_X, X)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # validation loss 계산.\n",
    "        if i % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                val_100_loss = []\n",
    "                for (X, _) in valid_loader:\n",
    "                    X = X.cuda()\n",
    "                    z = encoder(X)\n",
    "                    recon_X = decoder(z)\n",
    "                    loss = loss_function(recon_X, X)\n",
    "\n",
    "                    val_100_loss.append(loss)\n",
    "                train_loss_list.append(loss)\n",
    "                val_loss_list.append(np.asarray(val_100_loss).sum() / len(valid_loader))\n",
    "        print(\"[%d/%d][%d/%d] loss : %f\" % (i, len(train_loader), epoch, epochs, loss))\n",
    "\n",
    "# 학습된 모델의 weight를 저장하는 코드\n",
    "project_root_path = '/content/gdrive/My Drive/AI'\n",
    "encoder_save_path = '%s/pretrained_encoder.pth' % (project_root_path)\n",
    "torch.save(encoder.state_dict(), encoder_save_path)\n",
    "\n",
    "print(\"testing\")\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i, (X, _) in enumerate(test_loader):\n",
    "        X = X.cuda()\n",
    "        z = encoder(X)\n",
    "        recon_X = decoder(z)\n",
    "\n",
    "        print(\"오토인코더 테스트 결과\")\n",
    "        for i in range(5):\n",
    "            plt.imshow(X[i].cpu().reshape(28, 28))\n",
    "            plt.gray()\n",
    "            plt.show()\n",
    "\n",
    "            plt.imshow(recon_X[i].cpu().reshape(28, 28))\n",
    "            plt.gray()\n",
    "            plt.show()\n",
    "        break\n",
    "\n",
    "plt.plot(np.column_stack((train_loss_list, val_loss_list)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fcn = MNIST_FCN(class_num=10).cuda()\n",
    "fcn.apply(weight_init)\n",
    "\n",
    "# 저장해둔 weight를 불러와 해당 weight로 초기화 시킨다.\n",
    "pretrained_encoder = MNIST_CNN_Encoder().cuda()\n",
    "project_root_path = '/content/gdrive/My Drive/AI'\n",
    "encoder_save_path = '%s/pretrained_encoder.pth' % (project_root_path)\n",
    "saved_weights = torch.load(encoder_save_path)\n",
    "pretrained_encoder.load_state_dict(saved_weights)\n",
    "#pretrained_encoder.apply(weight_init) # 처음부터 학습하는 것을 테스트하고 싶을 경우\n",
    "\n",
    "epochs = 5\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "optimizer = optim.Adam(list(fcn.parameters())+list(pretrained_encoder.parameters()), betas=(0.5, 0.999), lr=learning_rate)\n",
    "#optimizer = optim.Adam(fcn.parameters(), betas=(0.5, 0.999), lr=learning_rate)  # Adam optimizer로 변경. betas =(0.5, 0.999) # encoder는 고정하고 fcn만 학습하는 코드\n",
    "\n",
    "train_loss_list = []\n",
    "fcn.train()\n",
    "for epoch in range(epochs):\n",
    "    for i, (X, t) in enumerate(train_loader):\n",
    "        X = X.cuda()\n",
    "        t = one_hot_embedding(t, 10).cuda()\n",
    "        z = pretrained_encoder(X)\n",
    "        Y = fcn(z)\n",
    "\n",
    "        loss = loss_function(Y, t)\n",
    "        train_loss_list.append(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"[%d/%d][%d/%d] loss : %f\"%(i,len(train_loader),epoch,epochs, loss))\n",
    "\n",
    "print(\"calculating accuracy...\")\n",
    "fcn.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i, (X, t) in enumerate(test_loader):\n",
    "        X = X.cuda()\n",
    "        t = one_hot_embedding(t, 10).cuda()\n",
    "        z = pretrained_encoder(X)\n",
    "        Y = fcn(z)\n",
    "\n",
    "        onehot_y= softmax_to_one_hot(Y)\n",
    "        correct += int(torch.sum(onehot_y * t))\n",
    "print(\"Accuracy : %f\" % (100. * correct / len(test_loader.dataset)))\n",
    "plt.plot(train_loss_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}