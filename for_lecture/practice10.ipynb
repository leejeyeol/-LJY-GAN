{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# **10. Pytorch를 이용한 MNIST classifier(CNN ver)**\n",
    "\n",
    "CNN을 사용하여 MNIST Classifier를 만들어봅시다.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MNIST_classifier_CNN(nn.Module):\n",
    "  def __init__(self, class_num):\n",
    "    super().__init__()\n",
    "    self.class_num = class_num\n",
    "\n",
    "    self.conv_net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5),\n",
    "        nn.BatchNorm2d(10),\n",
    "        nn.ReLU(), \n",
    "        nn.MaxPool2d(2),\n",
    "\n",
    "        nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5),\n",
    "        nn.BatchNorm2d(20),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "    self.fc_net = nn.Sequential(\n",
    "        nn.Linear(320,50),\n",
    "        nn.BatchNorm1d(50),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50,self.class_num),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    feature = self.conv_net(x)\n",
    "    feature = feature.view(-1,320)\n",
    "    y = self.fc_net(feature)\n",
    "    return y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    # 단일 라벨 텐서를 원핫 벡터로 바꿔줍니다.\n",
    "    y = torch.eye(num_classes)\n",
    "    one_hot = y[labels]\n",
    "    return one_hot\n",
    "\n",
    "def softmax_to_one_hot(tensor):\n",
    "    # softmax 결과를 가장 높은 값이 1이 되도록 하여 원핫 벡터로 바꿔줍니다. acuuracy 구할 때 씁니다.\n",
    "    max_idx = torch.argmax(tensor, 1, keepdim=True)\n",
    "    if tensor.is_cuda :\n",
    "        one_hot = torch.zeros(tensor.shape).cuda()\n",
    "    else:\n",
    "        one_hot = torch.zeros(tensor.shape)\n",
    "    one_hot.scatter_(1, max_idx, 1)\n",
    "    return one_hot\n",
    "\n",
    "def weight_init(m):\n",
    "    # Conv layer와 batchnorm layer를 위한 가중치 초기화를 추가함.\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear')!=-1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "epochs = 5\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# load the dataset\n",
    "dataset = datasets.MNIST('../data', train=True,\n",
    "                         download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "        , transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ]))\n",
    "num_train = len(dataset)\n",
    "valid_size = 500\n",
    "\n",
    "indices = list(range(num_train))\n",
    "split = num_train-valid_size\n",
    "np.random.shuffle(indices)\n",
    "train_idx, valid_idx = indices[:split], indices[split:]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                           batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                           batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor()\n",
    "                       ,transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "net = MNIST_classifier_CNN(class_num=10).cuda() # gpu 사용.(뒤에 .cuda())\n",
    "net.apply(weight_init)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), betas=(0.5, 0.999), lr=learning_rate)  # Adam optimizer로 변경. betas =(0.5, 0.999)\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "net.train()\n",
    "for epoch in range(epochs):\n",
    "    for i, (X, t) in enumerate(train_loader):\n",
    "        X = X.cuda() # gpu 사용.(뒤에 .cuda()) => view를 이용해 vectorize하는 부분 사라짐\n",
    "        t = one_hot_embedding(t, 10).cuda() # gpu 사용.(뒤에 .cuda())\n",
    "\n",
    "        Y = net(X)\n",
    "        loss = loss_function(Y, t)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #validation loss 계산. 계산이 무거우니 몇백 iteration혹은 몇 epoch마다 한번 수행하는것이 적당합니다. 예제는 매 100 iteration마다 수행합니다.\n",
    "        if i % 100 == 0: \n",
    "          with torch.no_grad():\n",
    "              val_100_loss = []\n",
    "              for (X, t) in valid_loader:\n",
    "                X = X.cuda() # gpu 사용.(뒤에 .cuda()) \n",
    "                t = one_hot_embedding(t, 10).cuda() # gpu 사용.(뒤에 .cuda())\n",
    "      \n",
    "                Y = net(X)\n",
    "                loss = loss_function(Y, t)\n",
    "                val_100_loss.append(loss)\n",
    "              train_loss_list.append(loss)\n",
    "              val_loss_list.append(np.asarray(val_100_loss).sum()/len(valid_loader))\n",
    "        print(\"[%d/%d][%d/%d] loss : %f\"%(i,len(train_loader),epoch,epochs, loss))\n",
    "\n",
    "print(\"calculating accuracy...\")\n",
    "net.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i, (X, t) in enumerate(test_loader):\n",
    "        X = X.cuda() # gpu 사용.(뒤에 .cuda())\n",
    "        t = one_hot_embedding(t, 10).cuda() # gpu 사용.(뒤에 .cuda())\n",
    "        Y = net(X)\n",
    "\n",
    "        onehot_y= softmax_to_one_hot(Y)\n",
    "        correct += int(torch.sum(onehot_y * t))\n",
    "print(\"Accuracy : %f\" % (100. * correct / len(test_loader.dataset)))\n",
    "plt.plot(np.column_stack((train_loss_list,val_loss_list)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}